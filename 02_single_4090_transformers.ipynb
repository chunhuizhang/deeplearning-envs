{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "973439e1",
   "metadata": {},
   "source": [
    "- 单位\n",
    "    - K：10^3, 1e3, 千，thousand\n",
    "    - M: 10^6, 1e6, 百万，million\n",
    "    - G: 10^9, 1e9, 10亿，billion\n",
    "    - T: 10^12, 1e12, 万亿，trillion\n",
    "- TFLOPS, TFLOPs\n",
    "    - TFLOPs：复数概念，多少个浮点数运算\n",
    "    - TFLOPS：速度概念，每秒多少个浮点数运算\n",
    "- transformer layer: BERT, GPT2, T5\n",
    "    - (multi head attn) + ffn\n",
    "    - multi head attn\n",
    "        - 兼容 self attention 和 cross attention\n",
    "        - 而 cross attn 只出现在 encoder + decoder 都有的情况\n",
    "- 参考（李沐大神）\n",
    "    - https://www.bilibili.com/video/BV1LT411F77M\n",
    "    - https://github.com/mli/transformers-benchmarks/blob/main/micro_bench.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c836970f",
   "metadata": {},
   "source": [
    "# Mirco-Benchmarking for Transformers\n",
    "\n",
    "This notebook benchmarks the most time consuming components in BERT, GPT-2 and T5 to help you understand its performance. Let's first check our libraries and hardware. If your GPUs are recent models, please make sure your CUDA version is also recent, which may greatly affect the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63b75c42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T13:39:16.425720Z",
     "start_time": "2023-05-10T13:39:15.152884Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version\t: 2.0.0+cu118\n",
      "CUDA version\t: 11.8\n",
      "GPU\t\t: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print('Pytorch version\\t:', torch.__version__)\n",
    "print('CUDA version\\t:', torch.version.cuda)\n",
    "print('GPU\\t\\t:',torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bcfd53",
   "metadata": {},
   "source": [
    "Let's first define a `walltime` method to benchmark Pytorch statements by at least 3 seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e53dca65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T13:39:41.028386Z",
     "start_time": "2023-05-10T13:39:39.800766Z"
    }
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from torch.utils import benchmark \n",
    "\n",
    "pd.options.display.precision = 3\n",
    "\n",
    "def var_dict(*args):\n",
    "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
    "    return dict([(name, val) for name, val in callers_local_vars if val is arg][0] \n",
    "                for arg in args)\n",
    "\n",
    "def walltime(stmt, arg_dict, duration=3):\n",
    "    return benchmark.Timer(stmt=stmt, globals=arg_dict).blocked_autorange(\n",
    "        min_run_time=duration).median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09d79ab",
   "metadata": {},
   "source": [
    "Last install huggingface from source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5f71a01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T13:40:51.754258Z",
     "start_time": "2023-05-10T13:40:37.869779Z"
    }
   },
   "outputs": [],
   "source": [
    "# 安装最新版本的 transformer（最新版本，源码安装）\n",
    "from IPython.display import clear_output\n",
    "\n",
    "!git clone git@github.com:huggingface/transformers.git\n",
    "!cd transformers; pip install .\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58f15d48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T13:41:18.339239Z",
     "start_time": "2023-05-10T13:41:18.185694Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.30.0.dev0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72fb575",
   "metadata": {},
   "source": [
    "## Matrix Multiplication\n",
    "\n",
    "Matrix multiplication is the most used operator in Transformers. Its performance is crucial. Let's test the [TFLOPS](https://en.wikipedia.org/wiki/FLOPS) we can achieve on square matrices. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0201a0",
   "metadata": {},
   "source": [
    "- TFLOPS：每s运行了多少次 tf（浮点运算），速度概念\n",
    "    - TFLOPs：复数的概念\n",
    "- $c_{n\\cdot n}=a_{n\\cdot n}\\cdot b_{n\\cdot n}$\n",
    "    - 我们从结果（$c_{n\\cdot n}$）出发，它的每一个位置（entry），都是由 $n$ 次乘法 + $n$ 次加法（准确地说是 $n-1$ 次加法）组成（矢量内积）\n",
    "        - n+(n-1) = 2n-1 == 2n\n",
    "    - $(n+n)\\cdot n\\cdot n=2n^3$\n",
    "- 更高的 tflops：更大的矩阵乘法，float32 => float16\n",
    "- float16\n",
    "    - cuBLAS，使用 tensor cores；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9709395e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T13:47:22.430421Z",
     "start_time": "2023-05-10T13:43:41.628901Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [03:40<00:00, 31.54s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n=128</th>\n",
       "      <th>n=512</th>\n",
       "      <th>n=2048</th>\n",
       "      <th>n=4096</th>\n",
       "      <th>n=8192</th>\n",
       "      <th>n=16384</th>\n",
       "      <th>n=32768</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>torch.float32</th>\n",
       "      <td>0.592</td>\n",
       "      <td>24.036</td>\n",
       "      <td>53.795</td>\n",
       "      <td>49.005</td>\n",
       "      <td>52.182</td>\n",
       "      <td>51.423</td>\n",
       "      <td>45.631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>torch.float16</th>\n",
       "      <td>0.573</td>\n",
       "      <td>35.177</td>\n",
       "      <td>164.255</td>\n",
       "      <td>166.949</td>\n",
       "      <td>156.083</td>\n",
       "      <td>173.988</td>\n",
       "      <td>172.340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               n=128   n=512   n=2048   n=4096   n=8192  n=16384  n=32768\n",
       "torch.float32  0.592  24.036   53.795   49.005   52.182   51.423   45.631\n",
       "torch.float16  0.573  35.177  164.255  166.949  156.083  173.988  172.340"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dict of dict\n",
    "from tqdm import tqdm\n",
    "matmul_tflops = defaultdict(lambda: {})\n",
    "for n in tqdm([128, 512, 2*1024, 4*1024, 8*1024, 16*1024, 32*1024]):\n",
    "    for dtype in (torch.float32, torch.float16):\n",
    "        a = torch.randn(n, n, dtype=dtype).cuda()\n",
    "        b = torch.randn(n, n, dtype=dtype).cuda()   \n",
    "        t = walltime('a @ b', var_dict(a, b))\n",
    "        matmul_tflops[f'n={n}'][dtype] = 2*n**3 / t / 1e12\n",
    "        del a, b\n",
    "        \n",
    "pd.DataFrame(matmul_tflops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26669333",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T15:37:33.721259Z",
     "start_time": "2023-04-11T15:37:33.465063Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f7d43b3caf0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "xs = [128, 512, 2*1024, 4*1024, 8*1024, 16*1024, 32*1024]\n",
    "plt.plot(xs, list(map(lambda x: matmul_tflops[f'n={x}'][torch.float32], xs)))\n",
    "plt.plot(xs, list(map(lambda x: matmul_tflops[f'n={x}'][torch.float16], xs)))\n",
    "plt.legend(['float32', 'float16'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e1c2eb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T13:49:13.516112Z",
     "start_time": "2023-05-10T13:49:13.508479Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 1.279127829560586\n",
      "torch.float16 2.1397053397938857\n"
     ]
    }
   ],
   "source": [
    "print('torch.float32', 53.795/42.056)\n",
    "print('torch.float16', 173.988/81.314)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157d3ac3",
   "metadata": {},
   "source": [
    "You can see that the performance increases with the matrix size. If your GPU has [Tensor Cores](https://www.nvidia.com/en-us/data-center/tensor-cores/), you will see a big performance jump when switching from 32-bit floating points to 16-bit floating points.\n",
    "\n",
    "Next you can find the theory TFLOPS of your GPU from Wikipedia, for example, [Nvidia Tesla](https://en.wikipedia.org/wiki/Ampere_(microarchitecture)), [Nvidia Quadro](https://en.wikipedia.org/wiki/Quadro), [RTX 40xx](https://en.wikipedia.org/wiki/GeForce_40_series), [RTX 30xx](https://en.wikipedia.org/wiki/GeForce_30_series), and [RTX 20xx](https://en.wikipedia.org/wiki/GeForce_20_series). Here we list several cards, with their memory information.\n",
    "\n",
    "| Model       | Memory (GB) | Memory Bandwidth (GB/sec) | FP32 TFLOPS | FP16 TFLOPS |\n",
    "| ----------- | ----------- | ------------------------- | ----------- | ----------- |\n",
    "| A100        | 80          | 2039                      | 19.5        | 312         |\n",
    "| V100        | 16          | 900                       | 15.7        | 125         |\n",
    "| A6000       | 48          | 768                       | 38          | 150         |\n",
    "| RTX 3090 TI | 24          | 1008                      | 40          | 160         |\n",
    "| RTX 4090    | 24          | 1008                      | 82          | 330         |\n",
    "\n",
    "\n",
    "If the best TFLOPS number you got is still far away from the theory TFLOPS of your GPU, the performance is likely bottlenecked by the memory bandwidth. To illustrate it, let's benchmark a simple elemental-wise multiplication to show both its TFLOPS with memory bandwidth. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4e85c6",
   "metadata": {},
   "source": [
    "- 深度学习中的按元素（element wise）运算：\n",
    "    - 一个layer的输出，经过 activate function；\n",
    "    - 权重的更新；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef74628e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T15:30:09.808674Z",
     "start_time": "2023-05-10T15:29:48.284417Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>65536</th>\n",
       "      <th>262144</th>\n",
       "      <th>1048576</th>\n",
       "      <th>4194304</th>\n",
       "      <th>16777216</th>\n",
       "      <th>67108864</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TFLOPS</th>\n",
       "      <td>0.009</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GB/s</th>\n",
       "      <td>70.541</td>\n",
       "      <td>343.917</td>\n",
       "      <td>1385.415</td>\n",
       "      <td>3777.138</td>\n",
       "      <td>920.339</td>\n",
       "      <td>921.202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        65536     262144    1048576   4194304   16777216  67108864\n",
       "TFLOPS     0.009     0.043     0.173     0.472     0.115     0.115\n",
       "GB/s      70.541   343.917  1385.415  3777.138   920.339   921.202"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = defaultdict(lambda: {})\n",
    "# *4\n",
    "for n in [1024*64, 1024*256, 1024*1024, 1024*1024*4, 1024*1024*16, 1024*1024*64]:\n",
    "    a = torch.randn(n).cuda()\n",
    "    t = walltime('a * 1.2', var_dict(a))\n",
    "    vector[n]['TFLOPS'] = n / t / 1e12\n",
    "    # float32: 4 Byte;\n",
    "    # 读写：两个操作；\n",
    "    vector[n]['GB/s'] = (4*2) * n / t / 1e9\n",
    "    \n",
    "pd.DataFrame(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30245ea",
   "metadata": {},
   "source": [
    "You can see that even for large vectors, the TFLOPS is far far way from GPU peak performance, while the bandwidth may be quite close to its theoretical number.\n",
    "\n",
    "The matrix multiplication performance is a main topic in HPC. There are a large number of research papers. Unfortunately the backend library, cuBLAS, is not open sourced. You may check [cutlass](https://github.com/NVIDIA/cutlass), which claimed similar performance as cuBLAS, for some implementation details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f832506",
   "metadata": {},
   "source": [
    "## BERT Layer\n",
    "\n",
    "The main body of a Transformer model is a stacking of Transformer blocks. Let's benchmark the performance of a single block. In BERT, it is often called a BERT layer. Let's construct one such layer from the [BERT large model](https://huggingface.co/bert-large-uncased). We use 16-bit floating points for better performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4245b00d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T14:00:12.415947Z",
     "start_time": "2023-05-10T14:00:11.142383Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, BertLayer\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"bert-large-uncased\")\n",
    "layer = BertLayer(config).half().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1dfe1ce9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T15:31:28.394605Z",
     "start_time": "2023-05-10T15:31:28.386741Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"bert-large-uncased\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 1024,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 4096,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 16,\n",
       "  \"num_hidden_layers\": 24,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.30.0.dev0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91da8378",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T14:36:14.727557Z",
     "start_time": "2023-05-10T14:36:14.720213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "# multihead attention: 64*16\n",
    "print(config.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec6d136",
   "metadata": {},
   "source": [
    "Then define a function to benchmark both forward and forward with backward performance using different sequence lengths and batch sizes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ebc3fb",
   "metadata": {},
   "source": [
    "- input_shape: (b, s, h)\n",
    "- ffn：\n",
    "    - 两层 mlp，h=>4h=>h\n",
    "        - h->4h\n",
    "            - (b, h)\\*(h, 4h) => (b, 4h)\n",
    "            - (b\\*4h)(2\\*h) == 8\\*b\\*h\\*h\n",
    "        - 4h->h\n",
    "            - (b, 4h)\\*(4h, h) => (b, h)\n",
    "            - (b\\*h)\\*(2\\*4\\*h) == 8\\*b\\*h\\*h\n",
    "        - 16\\*b\\*h\\*h\n",
    "        - 16\\*b\\*s\\*h\\*h\n",
    "- attn：假如有 n 个头，每个头的维度：h/n（Q，K，V）\n",
    "    - 三步\n",
    "       - 第一步先做投影，\n",
    "           - Q: (s, h) * (h, h/n) ==> (s, h/n)\n",
    "               - s\\*(h/n)\\*(2h)\n",
    "           - K: (s, h) * (h, h/n) ==> (s, h/n)\n",
    "               - s\\*(h/n)\\*(2h)\n",
    "           - V: (s, h) * (h, h/n) ==> (s, h/n)\n",
    "               - s\\*(h/n)\\*(2h)\n",
    "           - s\\*(h/n)\\*(2h)\\*3 = 6\\*(h\\*h/n)\\*s\n",
    "       - 再计算 attn_score: (Q\\*K^T)\\*V\n",
    "           - (s, h/n) \\* (h/n, s) => (s, s)\n",
    "               - s\\*s*(2h/n)\n",
    "           - (s,s)\\*(s, h/n) => (s, h/n)\n",
    "               - (s\\*h/n)*(2s)\n",
    "           - s\\*s\\*(2h/n) + (s\\*h/n)\\*(2s) = 4\\*(h/n)\\*s\\*s\n",
    "       - n个(h/n) concat 为 h，做一次投影 (s, h) => (s, h)\n",
    "           - (6\\*(h\\*h/n)\\*s + 4\\*(h/n)\\*s\\*s) * n = 6\\*s\\*h\\*h + 4\\*h\\*s\\*s\n",
    "           - (s, h) \\* (h, h) => (s, h) \n",
    "               - s\\*h\\*(2\\*h) = 2\\*s\\*h\\*h\n",
    "       - 6\\*s\\*h\\*h + 4\\*h\\*s\\*s + 2\\*s\\*h\\*h = 8\\*s\\*h\\*h + 4\\*h\\*s\\*s\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fdd1ed1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T15:48:17.535774Z",
     "start_time": "2023-04-11T15:48:17.527977Z"
    }
   },
   "outputs": [],
   "source": [
    "def layer_benchmark(layer, hidden_size, seq_lens, batch_sizes, cross_attention=False):\n",
    "    h = hidden_size\n",
    "    results = defaultdict(lambda: {})    \n",
    "    encoder_state = 'encoder_hidden_states=X' if cross_attention else ''\n",
    "    for s in seq_lens:\n",
    "        for b in batch_sizes:            \n",
    "            ffn = 16*b*s*h*h / 1e12  # TFLOPs for the Feed-Forward Network\n",
    "            atten = (4*b*h*s*s + 8*b*s*h*h) / 1e12  # TFLOPs for attention            \n",
    "            forward = ffn + (2 if cross_attention else 1) * atten\n",
    "            \n",
    "            X = torch.randn(b, s, h).half().cuda()\n",
    "            results[f'batch={b}'][f'fwd seq_len={s}'] = forward / walltime(\n",
    "                f'layer(X, {encoder_state})', var_dict(layer, X))\n",
    "            results[f'batch={b}'][f'fwd+bwd seq_len={s}'] = 3 * forward / walltime(\n",
    "                f'layer(X, {encoder_state})[0].sum().backward()', var_dict(layer, X))            \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3639975b",
   "metadata": {},
   "source": [
    "In BERT pre-training, we often train with a sequence of 128 (stage 1) or 512 (stage 2). Let's test its performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13754beb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T15:50:08.388493Z",
     "start_time": "2023-04-11T15:48:22.895119Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch=2</th>\n",
       "      <th>batch=4</th>\n",
       "      <th>batch=8</th>\n",
       "      <th>batch=16</th>\n",
       "      <th>batch=32</th>\n",
       "      <th>batch=64</th>\n",
       "      <th>batch=128</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fwd seq_len=128</th>\n",
       "      <td>11.511</td>\n",
       "      <td>13.321</td>\n",
       "      <td>45.993</td>\n",
       "      <td>53.099</td>\n",
       "      <td>107.170</td>\n",
       "      <td>110.394</td>\n",
       "      <td>97.590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fwd+bwd seq_len=128</th>\n",
       "      <td>3.129</td>\n",
       "      <td>6.341</td>\n",
       "      <td>12.523</td>\n",
       "      <td>25.068</td>\n",
       "      <td>49.649</td>\n",
       "      <td>99.831</td>\n",
       "      <td>102.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fwd seq_len=512</th>\n",
       "      <td>29.852</td>\n",
       "      <td>82.675</td>\n",
       "      <td>76.396</td>\n",
       "      <td>73.583</td>\n",
       "      <td>71.270</td>\n",
       "      <td>68.964</td>\n",
       "      <td>69.280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fwd+bwd seq_len=512</th>\n",
       "      <td>13.490</td>\n",
       "      <td>26.978</td>\n",
       "      <td>53.157</td>\n",
       "      <td>80.533</td>\n",
       "      <td>76.346</td>\n",
       "      <td>78.427</td>\n",
       "      <td>78.398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     batch=2  batch=4  batch=8  batch=16  batch=32  batch=64  \\\n",
       "fwd seq_len=128       11.511   13.321   45.993    53.099   107.170   110.394   \n",
       "fwd+bwd seq_len=128    3.129    6.341   12.523    25.068    49.649    99.831   \n",
       "fwd seq_len=512       29.852   82.675   76.396    73.583    71.270    68.964   \n",
       "fwd+bwd seq_len=512   13.490   26.978   53.157    80.533    76.346    78.427   \n",
       "\n",
       "                     batch=128  \n",
       "fwd seq_len=128         97.590  \n",
       "fwd+bwd seq_len=128    102.060  \n",
       "fwd seq_len=512         69.280  \n",
       "fwd+bwd seq_len=512     78.398  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_benchmark(layer, config.hidden_size, [128, 512], [2, 4, 8, 16, 32, 64, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c19e1a44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T15:40:16.674248Z",
     "start_time": "2023-05-10T15:40:16.664885Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9542911768871265"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "110.394/56.488"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc66dab5",
   "metadata": {},
   "source": [
    "No surprise that a large batch size helps. But the best number is below the matrix multiplication TFLOPS. Let's find why.\n",
    "\n",
    "We first benchmark the first dense layer in the Feed-Forward Network (FFN) in the layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "584ead2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T15:40:50.553770Z",
     "start_time": "2023-05-10T15:40:50.543257Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1024, out_features=4096, bias=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ffn 中的其中一层 mlp, h=>4h\n",
    "layer.intermediate.dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ecfe68f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T15:56:32.543187Z",
     "start_time": "2023-04-11T15:56:28.579208Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dense layer TFLOPS: 160.980'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h, b, s = config.hidden_size, 64, 128\n",
    "X = torch.randn(b, s, h).half().cuda()\n",
    "\n",
    "'Dense layer TFLOPS: %.3f' % (8*b*s*h*h / 1e12 / walltime(    \n",
    "    'layer.intermediate.dense(X)', var_dict(layer, X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d31432",
   "metadata": {},
   "source": [
    "The number is pretty good. Then run this dense layer with the GeLU activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efc7665c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T14:36:31.809254Z",
     "start_time": "2023-05-10T14:36:31.800112Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertIntermediate(\n",
       "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "  (intermediate_act_fn): GELUActivation()\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ffn 中的其中一层 mlp\n",
    "layer.intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57fb2656",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T15:56:54.020838Z",
     "start_time": "2023-04-11T15:56:50.117131Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dense+Activation TFLOPS: 126.240'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Dense+Activation TFLOPS: %.3f' % (8*b*s*h*h / 1e12 / walltime(\n",
    "    'layer.intermediate(X)', var_dict(layer, X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6950e5",
   "metadata": {},
   "source": [
    "Even the activation function has a ignorable complexity, it brings down the TFLOPS. We pointed out the reason before, the elemental-wise operation of the activation function is bounded by the memory bandwidth.\n",
    "\n",
    "Now test the whole FFN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2770aff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T15:57:22.399093Z",
     "start_time": "2023-04-11T15:57:19.217411Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FFN TFLOPS: 135.765'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn = 16*b*s*h*h / 1e12\n",
    "'FFN TFLOPS: %.3f'%(ffn / walltime(\n",
    "    'layer.output(layer.intermediate(X),X)', var_dict(layer, X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe03c6b",
   "metadata": {},
   "source": [
    "The other part in the BERT layer is the multi-head self-attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "810add3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T15:57:29.209553Z",
     "start_time": "2023-04-11T15:57:26.062729Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Attention TFLOPS: 81.950'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att = (4*b*h*s*s + 8*b*s*h*h) / 1e12\n",
    "'Attention TFLOPS: %.3f'%(\n",
    "    att / walltime('layer.attention(X)', var_dict(layer, X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b6d5d0",
   "metadata": {},
   "source": [
    "Even though the main computation part of the attention block is still matrix multiplication, it has more memory bounded operators compared to FFN. So you see a lower TFLOPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "50edaac4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T15:57:49.815328Z",
     "start_time": "2023-04-11T15:57:49.807729Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53125"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att / ffn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6fc072",
   "metadata": {},
   "source": [
    "The ratio of complexity between attention and FFN depends on the BERT configuration. The overall performance is a weighted sum between the FLOPS of these two components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20680c24",
   "metadata": {},
   "source": [
    "## GPT-2 Block\n",
    "\n",
    "Next let's evaluate `gpt2-medium`, which has a similar architecture has `bert-large`, i.e. 24 layers with a 1024 hidden size. GPT2 is trained with a 1024 sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41b59ff0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T15:59:23.590267Z",
     "start_time": "2023-04-11T15:57:56.201822Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a533006246d4411a4f95b25e7bbb753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch=2</th>\n",
       "      <th>batch=4</th>\n",
       "      <th>batch=8</th>\n",
       "      <th>batch=16</th>\n",
       "      <th>batch=32</th>\n",
       "      <th>batch=64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fwd seq_len=512</th>\n",
       "      <td>25.072</td>\n",
       "      <td>49.734</td>\n",
       "      <td>56.900</td>\n",
       "      <td>49.412</td>\n",
       "      <td>48.346</td>\n",
       "      <td>47.935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fwd+bwd seq_len=512</th>\n",
       "      <td>12.614</td>\n",
       "      <td>25.118</td>\n",
       "      <td>49.785</td>\n",
       "      <td>54.885</td>\n",
       "      <td>53.958</td>\n",
       "      <td>54.169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fwd seq_len=1024</th>\n",
       "      <td>44.208</td>\n",
       "      <td>43.629</td>\n",
       "      <td>39.372</td>\n",
       "      <td>38.740</td>\n",
       "      <td>38.568</td>\n",
       "      <td>38.427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fwd+bwd seq_len=1024</th>\n",
       "      <td>27.067</td>\n",
       "      <td>44.980</td>\n",
       "      <td>44.579</td>\n",
       "      <td>43.975</td>\n",
       "      <td>44.094</td>\n",
       "      <td>44.113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      batch=2  batch=4  batch=8  batch=16  batch=32  batch=64\n",
       "fwd seq_len=512        25.072   49.734   56.900    49.412    48.346    47.935\n",
       "fwd+bwd seq_len=512    12.614   25.118   49.785    54.885    53.958    54.169\n",
       "fwd seq_len=1024       44.208   43.629   39.372    38.740    38.568    38.427\n",
       "fwd+bwd seq_len=1024   27.067   44.980   44.579    43.975    44.094    44.113"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Block\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"gpt2-medium\")\n",
    "layer = GPT2Block(config, layer_idx=0).half().cuda()\n",
    "layer_benchmark(layer, config.n_embd, [512, 1024], [2, 4, 8, 16, 32, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30023c0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T15:43:49.240692Z",
     "start_time": "2023-05-10T15:43:49.230310Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5548572209318212"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "56.900/36.595"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d8984e",
   "metadata": {},
   "source": [
    "You can see that, despite GPT-2 and BERT has the same complexity, GPT-2 has slightly worse TFLOPS when using the same batch size and sequence length. Also using a larger sequence length 1024 further harms the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41d9229",
   "metadata": {},
   "source": [
    "## T5 Layer\n",
    "\n",
    "T5 has both encoder and decoder, let's first benchmark the decoder, whose performance is similar to BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1160220f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T16:00:46.218081Z",
     "start_time": "2023-04-11T15:59:55.596166Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4899874efb1a4f1f9341935403b85b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch=2</th>\n",
       "      <th>batch=4</th>\n",
       "      <th>batch=8</th>\n",
       "      <th>batch=16</th>\n",
       "      <th>batch=32</th>\n",
       "      <th>batch=64</th>\n",
       "      <th>batch=128</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fwd seq_len=512</th>\n",
       "      <td>19.052</td>\n",
       "      <td>50.302</td>\n",
       "      <td>47.720</td>\n",
       "      <td>45.154</td>\n",
       "      <td>43.313</td>\n",
       "      <td>41.821</td>\n",
       "      <td>41.524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fwd+bwd seq_len=512</th>\n",
       "      <td>10.798</td>\n",
       "      <td>21.681</td>\n",
       "      <td>41.511</td>\n",
       "      <td>52.429</td>\n",
       "      <td>49.602</td>\n",
       "      <td>49.603</td>\n",
       "      <td>49.468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     batch=2  batch=4  batch=8  batch=16  batch=32  batch=64  \\\n",
       "fwd seq_len=512       19.052   50.302   47.720    45.154    43.313    41.821   \n",
       "fwd+bwd seq_len=512   10.798   21.681   41.511    52.429    49.602    49.603   \n",
       "\n",
       "                     batch=128  \n",
       "fwd seq_len=512         41.524  \n",
       "fwd+bwd seq_len=512     49.468  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.models.t5.modeling_t5 import T5Block\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"t5-large\")\n",
    "config.use_cache = False\n",
    "config.is_decoder = False\n",
    "config.is_encoder_decoder = False\n",
    "\n",
    "encoder = T5Block(config).half().cuda()\n",
    "layer_benchmark(encoder, config.d_model, [512], [2, 4, 8, 16, 32, 64, 128])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e6e76c",
   "metadata": {},
   "source": [
    "The decoder has an additional cross attention, which increases the time complexity and also hurts TFLOPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "34db39b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T16:01:44.690262Z",
     "start_time": "2023-04-11T16:00:56.210528Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch=2</th>\n",
       "      <th>batch=4</th>\n",
       "      <th>batch=8</th>\n",
       "      <th>batch=16</th>\n",
       "      <th>batch=32</th>\n",
       "      <th>batch=64</th>\n",
       "      <th>batch=128</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fwd seq_len=512</th>\n",
       "      <td>29.277</td>\n",
       "      <td>40.767</td>\n",
       "      <td>38.341</td>\n",
       "      <td>36.989</td>\n",
       "      <td>35.458</td>\n",
       "      <td>34.330</td>\n",
       "      <td>34.084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fwd+bwd seq_len=512</th>\n",
       "      <td>9.257</td>\n",
       "      <td>18.400</td>\n",
       "      <td>36.701</td>\n",
       "      <td>42.897</td>\n",
       "      <td>40.398</td>\n",
       "      <td>40.718</td>\n",
       "      <td>40.643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     batch=2  batch=4  batch=8  batch=16  batch=32  batch=64  \\\n",
       "fwd seq_len=512       29.277   40.767   38.341    36.989    35.458    34.330   \n",
       "fwd+bwd seq_len=512    9.257   18.400   36.701    42.897    40.398    40.718   \n",
       "\n",
       "                     batch=128  \n",
       "fwd seq_len=512         34.084  \n",
       "fwd+bwd seq_len=512     40.643  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.is_decoder = True\n",
    "decoder = T5Block(config).half().cuda()\n",
    "layer_benchmark(decoder, config.d_model, [512], [2, 4, 8, 16, 32, 64, 128], cross_attention=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b329888f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "To conclude, to achieve the best performance for a Transformer layer, you need to use a fast data type and a large batch size. For further improvement, we may need to rewrite the code. For example, [fusing](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#fuse-pointwise-operations) multiple kernels into a single one. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "336px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
